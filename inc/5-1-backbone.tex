\subsection{Backbone}

Как говорилось выше, базовая сеть может быть реализована по разному. Так как от ее выходных призноков зависят предсказания остальных частей детектора -- необходимо выбрать наиболее подходящую архитектуру для backbone-СНС.

Для этого были проанализированы исследования последних лет. В качестве сравнительных характеристик были выбраны accuracy и времени работы, а в качестве обучающей выборки -- ImageNet. Yа изображение ниже приведены результаты исследований:

\addimghere{5-1-resnet-benchmark}{1}{Сравнение различных СНС на ImageNet 2020}{retinanet-architecture}

%todo: add instanet and https://github.com/PaddlePaddle/PaddleClas%

Представленная еще в 2015 году Microsoft Research архитектура ResNet оказалась настолько удачной что и по сей день периодически ставит рекорды в области распознования изображений. Рассмотрим семейство архитектур ResNet побробнее.

Ее успех заключается в применении Residual блоков. Согласно уневерсальной теореме аппроксимации с ростом числа слоев в нейронной сети все острее встает проблема паралича нейронной сети -- из-за обратного распространения ошибки градиент от слоя к слою постоянно уменьшается. В результате более глубокие слои перестают обучаться и как следствие качество распознования падает:

\addimghere{5-1-cnn-back-propagation-paralysis}{0.8}{Увеличение колличества слоев дает ходший результат}{back-propagation-paralysis}

Основная идея ResNet заключается в том, чтобы ввести так называемое «соединение с пропусками» (Skip Connection), которое пропускает один или несколько уровней, как показано на рисунке ниже:

\addimghere{5-1-residual-block}{0.6}{Residual блок}{residual-block}

Благодоря Residual блокам градиент не умеьшается, а слои СНС можно бъеденять в длинные последовательности, увеличивая как обобщающую способность, так и точность такой сети:

\addimghere{5-1-resnet-training-results}{1}{Кривые обучения ResNet (справа) с 18 и 34 слоями в сравнении с аналогичной СНС без residual обоков (слева).}{residual-block}