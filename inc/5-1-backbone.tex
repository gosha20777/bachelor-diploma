\subsubsection{Backbone}

Как говорилось выше, базовая сеть может быть реализована по разному. Так как от ее выходных признаков зависят предсказания остальных частей детектора -- необходимо выбрать наиболее подходящую архитектуру для backbone-СНС.

Для этого были проанализированы исследования последних лет. В качестве сравнительных характеристик были выбраны accuracy и времени работы, а в качестве обучающей выборки -- ImageNet. На изображение ниже приведены результаты исследований:

\addimghere{5-1-resnet-benchmark}{1}{Сравнение различных СНС на ImageNet 2020}{resnet-benchmark}

Представленная еще в 2015 году Microsoft Research архитектура ResNet \cite{lib-resnet} оказалась настолько удачной, что и по сей день периодически ставит рекорды в области распознавания изображений. Рассмотрим семейство архитектур ResNet подробнее.

Ее успех заключается в применении Residual блоков. С ростом числа слоев в нейронной сети все острее встает проблема ее паралича -- во время обратного распространения ошибки градиент от слоя к слою постоянно уменьшается. В результате, более глубокие слои перестают обучаться, и, как следствие, качество распознавания падает:

\addimghere{5-1-cnn-back-propagation-paralysis}{0.8}{Увеличение колличества слоев дает ходший результат}{back-propagation-paralysis}

Основная идея ResNet заключается в том, чтобы ввести так называемое "соединение с пропусками" (Skip Connection), которое пропускает один или несколько уровней, как показано на рисунке ниже:

\addimghere{5-1-residual-block}{0.6}{Residual блок}{residual-block}

Благодаря Residual блокам, градиент не уменьшается, а слои СНС можно объединять в длинные последовательности, увеличивая как обобщающую способность, так и точность такой сети:

\addimghere{5-1-resnet-training-results}{1}{Кривые обучения ResNet (справа) с 18 и 34 слоями в сравнении с аналогичной СНС без residual обоков (слева)}{resnet-training-results}