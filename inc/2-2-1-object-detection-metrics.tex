\subsubsection{Метрики вценивания}

Попробуем ответить на вопрос как оценить качество работы нейронной сети? Самым очевидным будет определить долю правильных ответов (Ассuracy):
$$
accuracy = \frac{P}{N}
$$
здесь $P$ -- число правильных ответов а $N$ -- резмер выборки. Однако эта метрика бесполезна в задачах с неравными классами. Пусть имеются два класса $A$ и $B$. В выборке имеются 7 обьектов класса $A$ и 3 обьекта класса $B$. Теперь предположим что обученный алгоритм будет не способен отличать классы и относить любой обьект к классу $A$. Точность такого алгоритма будет равна $0.7$, что конечно неверно.

Обратимся к статистике и попробуем оценить качество алгоритма для каждого класса в отдельности. Введем такие метрики как $precision$ (точность) и $recall$ (полнота):
$$
precision = \frac{TP}{TP+FP};\ \ recall = \frac{TP}{TP+FN}
$$
где:
\begin{itemize}
    \item $TP\ (true\ posetive)$ -- число истино-положительных решений;
    \item $FP\ (false\ posetive)$ -- число ложно-положительных решений (ошибки I рода);
    \item $FN\ (false\ negative)$ -- число ложно-отрицательных решений (ошибки II рода).
\end{itemize}

С практической точки зрения $precision$ показывает долю объектов, названных классификатором положительными и при этом действительно являющимися положительными, а $recall$ -- какую долю объектов положительного класса из всех объектов положительного класса нашел алгоритм.

Теперь, когда мы имеем представления о $precision$ и $recall$, можем ввезти понятие average precision ($AP$). Он определяется как площадь под precision-recall кривой (PR кривой). Ось $X$ -- это $recall$, а ось $Y$ -- $precision$. Получение множетсва точек «precision-recall», размерность которого совпадает с числом ответов СНС, выполняется путем выбора порогового значения оценки. Любое обнаружение с оценкой ниже порогового значения рассматривается как ложное срабатывание. Для коджого класса, который присутствует в результатах обнаружения, мы вычисляем $precision$ и $recall$ в этой точке. После того, как мы построим все пары значений «precision-recall», соответствующие каждому уникальному порогу оценки, мы получим PR кривую. Пример такой кривой представлен на рисунке ниже:

\addimghere{2-2-1-pr-curve}{0.6}{PR кривая}{object-detection-output}

Рассмотрим еще одну метрику - $IoU$ (Intersection over Union). Она показывеет насколько точно удалось локализовать обьект и пределяется отношением площади пересечения и предсказанных и истинных boundong box-ов к их объединению (рис. \ref{iou-definition}). 

\addimghere{2-2-2-iou-definition}{0.4}{понятие IoU}{iou-definition}

Для оценивания качества детекторов часто применяют такие метрико как $AP@50$ или $AP@75$. Это нично иное как метрика $AP$ при пороговом значении $0.5$ и $0.75$ соответсвтенно метрики IoU. Если $IoU$ ниже заданного порога -- обьект не учитывается. 

Напоследок упомянем еще одну метрику -- $mAP$ (mean average precision). Как следует из названия это просто все значения $AP$, усредненные по разным классам. С практической точки зрения $mAP$ показывает долю объектов класса которую нашел алгоритм из всех объектов этого класса, однако в добавок учитывается уверенность модели в ложном варианте.