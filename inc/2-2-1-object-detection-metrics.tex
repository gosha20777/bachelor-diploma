\subsubsection{Метрики оценивания}

Попробуем ответить на вопрос: как оценить качество работы нейронной сети? Самым очевидным будет определить долю правильных ответов (Accuracy):
$$
accuracy = \frac{P}{N}
$$
здесь $P$ -- число правильных ответов а $N$ -- размер выборки. Однако, эта метрика бесполезна в задачах с неравными классами. Пусть, имеются два класса $A$ и $B$. В выборке имеются 7 объектов класса $A$ и 3 объекта класса $B$. Теперь предположим, что обученный алгоритм будет не способен отличать классы и относить любой объект к классу $A$. Точность такого алгоритма будет равна $0.7$, что конечно неверно.

Обратимся к статистике и попробуем оценить качество алгоритма для каждого класса в отдельности. Введем такие метрики как $precision$ (точность) и $recall$ (полнота) \cite{lib-ods-metrics}:
$$
precision = \frac{TP}{TP+FP};\ \ recall = \frac{TP}{TP+FN}
$$
где:
\begin{itemize}
    \item $TP\ (true\ posetive)$ -- число истинно-положительных решений;
    \item $FP\ (false\ posetive)$ -- число ложно-положительных решений (ошибки I рода);
    \item $FN\ (false\ negative)$ -- число ложно-отрицательных решений (ошибки II рода).
\end{itemize}

С практической точки зрения, $precision$ показывает долю объектов, названных классификатором положительными и при этом, действительно являющимися положительными, а $recall$ -- какую долю объектов положительного класса из всех объектов положительного класса нашел алгоритм.

Теперь, когда мы имеем представления о $precision$ и $recall$, можем ввезти понятие average precision ($AP$) \cite{lib-map-metric}. Она определяется как площадь под precision-recall кривой (PR кривой). Кривая строится на плоскости, где ось $X$ -- это $recall$, а ось $Y$ -- $precision$. Получение множества точек «precision-recall», размерность которого совпадает с числом ответов СНС, выполняется путем выбора порогового значения оценки. При этом любое обнаружение с оценкой ниже порогового значения рассматривается как ложное срабатывание. Для каждого класса, который присутствует в результатах обнаружения, мы вычисляем $precision$ и $recall$ в этой точке. После того, как мы отобразим все пары значений «precision-recall», соответствующие каждому уникальному порогу оценки, на плоскости, мы получим PR кривую. Пример такой кривой представлен на рисунке ниже:

\addimghere{2-2-1-pr-curve}{0.6}{PR кривая}{pr-curve}

Рассмотрим еще одну метрику - $IoU$ (Intersection over Union) \cite{lib-iou-metric}. Она показывает насколько точно удалось локализовать объект и определяется отношением площади пересечения предсказанных и истинных bounding box-ов к их объединению (рис. \ref{iou-definition}). 

\addimghere{2-2-2-iou-definition}{0.4}{понятие IoU}{iou-definition}

Для оценивания качества детекторов часто применяют такие метрики как $AP@50$ или $AP@75$. Это ничто иное как метрика $AP$ при пороговом значении $0.5$ и $0.75$ $IoU$ соответственно. Если $IoU$ ниже заданного порога -- объект не учитывается. 

Напоследок, упомянем еще одну метрику -- $mAP$ (mean average precision). Как следует из названия, это просто все значения $AP$, усредненные по разным классам. С практической точки зрения, $mAP$ показывает долю объектов класса, которую нашел алгоритм, из всех объектов этого класса, однако, в добавок, учитывается уверенность модели в ложном варианте.