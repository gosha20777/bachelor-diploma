\subsubsection{Оптимизация модели глубокого обучения} \label{sect-7-1}

Для возможности использования разработанной архитектуры WFPN на ПЭВМ и практического применения СНС в ПСО необходимо было как снизить ее требования к ресурсам, так и сократить время работы алгоритма или инференс (англ. inference) на портативных устройствах.

Для осуществления этих целей прибегают к следующим подходам:
\begin{itemize}
    \item Кванитизация -- это процесс уменьшения размера СНС в оперативной памяти компьютера. Уменьшение потребляемой памяти осуществляется путем уменьшения количества информации, необходимое для хранения одного параметра (веса) СНС. Так, например, изначально параметры модели имеют тип данных FP32. В процессе кванитизации тип данных может измениться на FP16 или даже INT8. Помимо снижения потребляемой памяти, кванитизация может привезти к уменьшению времени работы на различных устройствах. К примеру, на графических ускорителях nVidia модели с весами в формате FP16 могут выполняться до 2 раз быстрее, а кванитизация в INT8 ускоряет выполнение нейронных сетей на ARM устройствах;
    \item Объединение слоев СНС. Зачастую, некоторые слои СНС могут быть объеденины (например, свертка и ReLU активация), что приводит к увеличению производительности;
    \item Использование NPU (Neural Processor Unit) и TPU (Tensor Processor Unit) -- сопроцессоров, ускоряющих работу нейронных сетей;
    \item Использование аппаратно-ориентированных низкоуровневых библиотек, задействующих аппаратные возможности той, или иной платформы (например, AVX инструкции) для ускорения времени инференса.
\end{itemize}

Для практической реализации вышеупомянутых подходов мною были использованы следующие инструменты, библиотеки и устройства:

\begin{itemize}
    \item OneDNN (Tensorflow-oneDNN-2.3) -- низкоуровневая библиотека, обеспечивающая высокопроизводительное выполнение тензорных операций на x86 процессорах Intel и AMD \cite{lib-onednn};
    \item CUDA \cite{lib-cuda}, CuDNN \cite{lib-cudnn}, TensorRT \cite{lib-tensorrt} (Tensorflow-tensorrt-2.3) -- набор низкоуровневых библиотек и инструментов, обеспечивающие высокопроизводительное выполнение тензорных операций на графических ускорителях nVidia, а также объединение слоев и кванитизацию;
    \item Сопроцессоры Intel Movidius Myriad 2 \cite{lib-movidius} и Google Coral Edge TPU \cite{lib-coral} (рис. \ref{7-1-npus}) и соответствующие им наборы библиотек (OpenVINO \cite{lib-openvino} и Tensorflow-lite \cite{lib-tflite}).
\end{itemize}

Полученные результаты ускорения модели RetinaNet-WFPN приведены в таблице ниже:

\begin{table}[H]
    \caption{Оптимизация времени инференса RetinaNet-WFPN}\label{leaderboard-optilization}
    \begin{tabular}{|p{6cm}|c|c|p{2cm}|}
        \hline
        {Устройство} & {Библиотека} & {Формат} & {Время работы, мс} \\
        \hline
        Intel i7-9750H (6/12) @ 4.500GHz & Tensorflow-cpu-2.3 & FP32 & 1100 \\
        \hline
        Intel i7-9750H (6/12) @ 4.500GHz & Tensorflow-oneDNN-2.3 & FP16 & 800 \\
        \hline
        nVidia Quadro T1000 Mobile (4 Gb) & Tensorflow-gpu-2.3 & FP32 & 400 \\
        \hline
        nVidia Quadro T1000 Mobile (4 Gb) & Tensorflow-tensorrt-2.3 & FP16 & 300 \\
        \hline
        Intel i7-9750H (6/12) @ 4.500GHz + Intel Movidius Myriad 2 & OpenVINO & FP16 & 500 \\
        \hline
        Intel i7-9750H (6/12) @ 4.500GHz + Google Coral Edge TPU & Tensorflow-lite & INT8 & 240 \\
        \hline
    \end{tabular}
\end{table}

Из полученных результатов видно, что благодаря оптимизациям удалось уменьшить время работы алгоритма для CPU в 1.4 раза, для GPU в 1.3 раза. С применением сопроцессоров Intel Movidius Myriad 2 и Google Coral Edge TPU этот показатель составил 2.2 и 4.5 раза соответственно. В связи с этим, в дальнейшем для запуска СНС на ПЭВМ будем использовать библиотеки Tensorflow-oneDN для CPU, Tensorflow-tensorrt для GPU, а также, OpenVINO и Tensorflow-lite для сопроцессоров.

\addimghere{7-1-npus}{0.8}{Сопроцессоры Google Coral Edge TPU (сверху) и Intel Movidius Myriad 2 (снизу)}{7-1-npus}