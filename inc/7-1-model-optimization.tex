\subsection{Оптимизиция модели глубокого обучения} \label{sect-7-1}

Для возможности использования разработанной архитектуры WFPN на ПЭВМ и практического применения СНС в ПСО необходимо было как снизить ее требования к ресурсам, так и сократить пермя работы алгоритма или инференс (англ. inference) на портативнфх устройствах.

Для осущиствления этих целей прибегают к следующим подхожам:
\begin{itemize}
    \item кванитизация -- это процесс уменьшения размера СНС в оперативной памяти компьютера. Умешьшение потребляемой памяти оссуществляется путем уменьшения колличества информации необходимое для хранения одного параметра (веса) СНС. Так например, изначально параметры модели имеют тип данных FP32. В процессе кванитизации тип данных может измениться на FP16 или даже INT8. Помимо снижения потребляемой памяти кванитизация может привезти к уменьшению времени работы на различных устройствах. На графическиих ускорителях nVidia модели с весами в формате FP16 могут выполняться до 2 раз быстрее, а кванитизаций в INT8 ускоряет выполнение нейронных сетей на ARM устройствах;
    \item Объединение слоев СНС -- зачастую некоторые слои СНС могут быть объеденены (наппример свертка и ReLU активация), что приводит к увеличению производительности;
    \item Использование NPU (Neural Proceccor Unit) и TPU (Tensor Proceccor Unit) - сопроцессоров, ускоряющих работу нейронных сетей;
    \item Использование аппаратно-ориентированных низкоуровнивых библиотек, задействующих аппаратные возможности той или оний платформы (например AVX инструкции) для ускорения времени инференса.
\end{itemize}

Для практической реализации вышеупомянутых подходов мною были использованы следубщие инструменты, библиотеки и устройства:

\begin{itemize}
    \item OneDNN (Tensorflow-oneDNN-2.3) - низкоуровнивая библиотека обиспечивающая высокопроизводительное выполнение тензорных операций на x86 процессорах Intel и AMD;
    \item Cuda, CuDNN, TensorRT (Tensorflow-tensorrt-2.3) -- набор низкоуровневых библиотек и инструментов обиспечиваюие высокопроизводительное выполнение тензорных операций на графических ускорителях nVidia, а также объединение слоев и кванитизацию;
    \item Сопроцессоры Intel Movidius Myriad 2 и Google Coral Edge TPU (рис. \ref{7-1-npus}) и соответвтубщие им наборы библиотек (OpenVINO и Tensorflow-lite).
\end{itemize}

Полученные результаты ускорения модели RetinaNet-WFPN приведены в таблице ниже:

\begin{table}[H]
    \caption{Оптимизация времени инференса RetinaNet-WFPN}\label{leaderboard-full}
    \begin{tabular}{|p{6cm}|c|c|p{2cm}|}
        \hline
        {Устройство} & {Библиотека} & {Формат} & {Время работы, мс} \\
        \hline
        Intel i7-9750H (6/12) @ 4.500GHz & Tensorflow-cpu-2.3 & FP32 & 1100 \\
        \hline
        Intel i7-9750H (6/12) @ 4.500GHz & Tensorflow-oneDNN-2.3 & FP16 & 800 \\
        \hline
        nVidia Quadro T1000 Mobile (4 Gb) & Tensorflow-gpu-2.3 & FP32 & 400 \\
        \hline
        nVidia Quadro T1000 Mobile (4 Gb) & Tensorflow-tensorrt-2.3 & FP16 & 300 \\
        \hline
        Intel i7-9750H (6/12) @ 4.500GHz + Intel Movidius Myriad 2 & OpenVINO & FP16 & 500 \\
        \hline
        Intel i7-9750H (6/12) @ 4.500GHz + Google Coral Edge TPU & Tensorflow-lite & INT8 & 240 \\
        \hline
    \end{tabular}
\end{table}

Из полученных результатов видно что благодаря оптимизациям удалось уменьшить время работы алгоритма для CPU в 1.4 раза, для GPU в 1.3 раза. С применением сопроцессоров Intel Movidius Myriad 2 и Google Coral Edge TPU этот показатель составил 2.2 и 4.5 раза соответсвенно. В связи с этим в дальнейшем для запуска СНС на ПЭВМ будем использовать библиотеки Tensorflow-oneDN для CPU, Tensorflow-tensorrt для GPU, OpenVINO и Tensorflow-lite для сопроцессоров.

\addimghere{7-1-npus}{0.8}{Сопроцессоры Google Coral Edge TPU (сверху) и Intel Movidius Myriad 2 (снизу)}{7-1-npus}