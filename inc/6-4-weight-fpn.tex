\subsection{Взвешенная FPN}

В поисках более производительного решения, я проанализировал ряд научных статей, описывающих различные варианты модификаций FPN. Так, в одном из исследований \cite{lib-dfpn} FPN пирамида была углублена: более низкие уровни FPN были продублированы (рис. \ref{6-4-deep-fpn}). Такая конфигурации получила название Deep Feature Pyramid Network (DFPN). Основная идея DFPN заключается в том, что при повторении более низких уровней пирамиды увеличивается их вклад в предсказание СНС.

\addimghere{6-4-deep-fpn}{0.8}{Архитектурная схема DFPN}{6-4-deep-fpn}

Данное исследование натолкнуло меня на мысль о том, что различные уровни пирамиды признаков могут иметь для предсказания разное значение или вес. Как говорилось в разделе \ref{sect-5-2}, на каждом уровне нисходящей пирамиды сигнал складывается с соответствующим уровнем восходящей пирамиды, благодаря боковым соединениям. В оригинальной FPN эти сигналы имеют одинаковый, фиксированный вклад и результирующий сигнал равен их среднему значению, но это не оптимально. Действительно, на разных масштабах разные объекты могут представляться по разному. 

Для реализации этой идеи введем дополнительные усиливающие коэффициенты (веса) для каждого сигнала (рис. \ref{6-4-weight-fpn}) и будем вычислять результирующий сигнал с учетом этих коэффициентов $w^{j}_{i}$. Тогда признаки уровня $P_i$ будут вычисляться по следующей формуле:
$$
P_i = w^{C}_i \cdot C_i + w^{P}_i \cdot P_{i+1}
$$

Коэффициенты $w^{j}_{i}$ будем находить -- как и другие параметры СНС -- методом обратного распространения ошибки, нормализуя их в интервале $[0..1]$: $w^{j}_{i} = softmax(\Theta^{j}_{i})$. Во избежании больших значений обучаемых параметров $\Theta^{j}_{i}$ (и как следствие паралича СНС), будем применять к ним $L_2$ регуляризацию. Предлагаемая мной архитектура получила название Weight Feature Pyramid Network (WFPN).

\addimghere{6-4-weight-fpn}{0.4}{Схема Weight Feature Pyramid Network (WFPN)}{6-4-weight-fpn}

Для для проверки описанной выше гипотезы было произведено сравнение классической FPN с конфигурациями $P_3..P_7$ и $P_2..P_5$, DFPN конфигурации $P_3..P_7 + P_3..P_6 + ... + P_3$ и разработанной мной WFPN ($P_3..P_7$). Обучение моделей проводилось описанным в предыдущих разделах [\ref{sect-6-2}][\ref{sect-6-3}] способом. Размер входного изображения -- $2000\times1500$, обработка изображений проводилась на GPU nVidia Tesla V100.

\begin{table}[H]
    \caption{Сравнение конфигураций FPN}\label{leaderboard-full}
    \begin{tabular}{|c|c|c|c|}
        \hline
        {Тип модели} & {VisDrone, mAP} & {LaDD, mAP} & Время работы, мс \\
        \hline
        RetinaNet ($P_3..P_7$) & 0.34 & 0.87 & 400 \\
        \hline
        RetinaNet ($P_2..P_5$) & 0.42 & 0.90 & 1100 \\
        \hline
        RetinaNet-DFPN & 0.43 & 0.91 & 800 \\
        \hline
        RetinaNet-WFPN & 0.48 & 0.93 & 400 \\
        \hline
    \end{tabular}
\end{table}

Полученные результаты показывают, что Weight FPN не только демонстрирует лучшие результаты в задачи детектирования небольших объектов, но и работает с той же скоростью, что и классическая FPN. Также хочется отметить, что полученная архитектура является лучшей для задачи VisDrone (по метрике $mAP$). 

\addimghere{6-4-vis-drone-leaderboard}{1}{5 лучших решений задачи VisDrone}{6-4-vis-drone-leaderboard}

Полученные высокие результаты вполне можно объяснить и с точки зрения биологии: когда мы обронили что то маленькое и пытаемся это найти, мы фокусируемся на маленьких предметах и внимательно рассматриваем окружающее нас пространство. Введенные усиливающие коэффициенты в пирамиде признаков выполняют схожую функцию. Благодаря этому механизму, мы настраиваем внимание СНС, акцентируясь на мелких объектах, и находим баланс между размерностями карт признаков и их семантической грубиной (receptive field). К тому-же в WFPN веса непрерывны ($w^j_i\in[0..1]$), благодаря чему соотношение сигналов можно гибко изменять, за счет чего достигается преимущество предлагаемой архитектуры над DFPN.